{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fb296850f654f6384978e623242a75d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f90f63edba141d9a1435165045e3fb2",
              "IPY_MODEL_412094a3175b479e82253f43adf2945d",
              "IPY_MODEL_e4d985ca9eae4e38af416fd31e7eb8d7"
            ],
            "layout": "IPY_MODEL_bf6a525760a84c369cc6531f440f4952"
          }
        },
        "0f90f63edba141d9a1435165045e3fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71b362ff79d0443a89c944284757eb0a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bb707a104d5248f2b0768ba68e9cac4c",
            "value": "Batches:â€‡100%"
          }
        },
        "412094a3175b479e82253f43adf2945d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cdc0dd00bee4d2ebddb7bde4bc6fb19",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca218ac0825f43828dc4793caabe62d7",
            "value": 1
          }
        },
        "e4d985ca9eae4e38af416fd31e7eb8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ff3123acfe4545a1c4bbe14bab242c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f5ca1de4326d41979fc06c9d0de03711",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡3.49it/s]"
          }
        },
        "bf6a525760a84c369cc6531f440f4952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71b362ff79d0443a89c944284757eb0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb707a104d5248f2b0768ba68e9cac4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cdc0dd00bee4d2ebddb7bde4bc6fb19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca218ac0825f43828dc4793caabe62d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4ff3123acfe4545a1c4bbe14bab242c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ca1de4326d41979fc06c9d0de03711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e146462e98a4a81b18396b5f0a38466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9119df471c624dadad07d9d80796b136",
              "IPY_MODEL_47db453f1dc24134856c55f7883b6838",
              "IPY_MODEL_fc53bb56943b4ca88a0dbee4a349d8bb"
            ],
            "layout": "IPY_MODEL_d6ea73dee0ec4e5ba708edf20ceb7ac4"
          }
        },
        "9119df471c624dadad07d9d80796b136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aaac32129b34a6d9bf6a7e0a4f5e96c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0150921ec61c4736ae68b94e174aec3a",
            "value": "Batches:â€‡100%"
          }
        },
        "47db453f1dc24134856c55f7883b6838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ed490d95c04e95bdf4f437d67f26f6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7e216248da5423894bcc1f89b3f05fb",
            "value": 1
          }
        },
        "fc53bb56943b4ca88a0dbee4a349d8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_814123040e1e4757934f8d5e0cfe4e49",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b7595c975f4e4251a64e22e5db107478",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡33.61it/s]"
          }
        },
        "d6ea73dee0ec4e5ba708edf20ceb7ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aaac32129b34a6d9bf6a7e0a4f5e96c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0150921ec61c4736ae68b94e174aec3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70ed490d95c04e95bdf4f437d67f26f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e216248da5423894bcc1f89b3f05fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "814123040e1e4757934f8d5e0cfe4e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7595c975f4e4251a64e22e5db107478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Setup and Installation\n",
        "First, let's install all necessary packages"
      ],
      "metadata": {
        "id": "riBw_f9-IY4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jedi>=0.16\n",
        "# Install required packages\n",
        "!pip install -q gradio\n",
        "!pip install -q gradio_pdf\n",
        "!pip install -q pypdf PyPDF2 pymupdf\n",
        "!pip install -q sentence-transformers transformers\n",
        "!pip install -q faiss-cpu\n",
        "# !pip install -q google-generativeai\n",
        "!pip install -q numpy pandas\n",
        "\n",
        "# Install LlamaIndex packages for enhanced document processing\n",
        "!pip install -q llama-index\n",
        "!pip install -q llama-index-readers-file\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index-vector-stores-faiss\n",
        "# !pip install -q llama-index-llms-gemini"
      ],
      "metadata": {
        "id": "MEXBca59DFk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123"
      ],
      "metadata": {
        "id": "NmNnDeI9rgoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p models\n",
        "!wget -O models/mistral-7b-instruct-v0.2.Q4_K_M.gguf \\\n",
        "  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4KmQh2qrmgu",
        "outputId": "54944f86-82aa-41d4-93ba-1f136497dd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-29 09:44:16--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.34, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/65778ac662d3ac1817cc9201/865f5e4682dddb29c2e20270b2471a7590c83a414bbf1d72cf4c08fdff2eeca4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251229%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251229T094416Z&X-Amz-Expires=3600&X-Amz-Signature=cbe9630fdc25fa676b7ee4ebbb7f7a617ca0ef8544a956011fe6fcfc5a1e0645&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1767005056&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzAwNTA1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTc3OGFjNjYyZDNhYzE4MTdjYzkyMDEvODY1ZjVlNDY4MmRkZGIyOWMyZTIwMjcwYjI0NzFhNzU5MGM4M2E0MTRiYmYxZDcyY2Y0YzA4ZmRmZjJlZWNhNCoifV19&Signature=pWmcWyemqdlBFK9xqilpSX5tnMU3NCElXW27WtGot1EvtVmDYfPykUp56WkBrRCdZyoErRE9O9QVOvwdGT08D7vPOdYzafKExocYI9xnCQjHFklFxWImmz972supFoaRdlahzDipjOuCAmI8Qk4DvQBJV4NmteXxdYdTr-bVFu-0xE3rKy8e0hmxWlkKLTE%7EockYclZUE9QcPVmRJYQ3WTypYgFjNJaejIiG1RPmfvaWcbKNkwclK1XZzOZe--FPlvUQ8FGwaf02s68NLB%7ELfVQQmT7a9Kqm-T9xIPIoJqPxPunp0B5MK7pIqUwaF2YNmXLijL3LM1FgdXKPZecEaA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-29 09:44:16--  https://cas-bridge.xethub.hf.co/xet-bridge-us/65778ac662d3ac1817cc9201/865f5e4682dddb29c2e20270b2471a7590c83a414bbf1d72cf4c08fdff2eeca4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251229%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251229T094416Z&X-Amz-Expires=3600&X-Amz-Signature=cbe9630fdc25fa676b7ee4ebbb7f7a617ca0ef8544a956011fe6fcfc5a1e0645&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1767005056&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzAwNTA1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTc3OGFjNjYyZDNhYzE4MTdjYzkyMDEvODY1ZjVlNDY4MmRkZGIyOWMyZTIwMjcwYjI0NzFhNzU5MGM4M2E0MTRiYmYxZDcyY2Y0YzA4ZmRmZjJlZWNhNCoifV19&Signature=pWmcWyemqdlBFK9xqilpSX5tnMU3NCElXW27WtGot1EvtVmDYfPykUp56WkBrRCdZyoErRE9O9QVOvwdGT08D7vPOdYzafKExocYI9xnCQjHFklFxWImmz972supFoaRdlahzDipjOuCAmI8Qk4DvQBJV4NmteXxdYdTr-bVFu-0xE3rKy8e0hmxWlkKLTE%7EockYclZUE9QcPVmRJYQ3WTypYgFjNJaejIiG1RPmfvaWcbKNkwclK1XZzOZe--FPlvUQ8FGwaf02s68NLB%7ELfVQQmT7a9Kqm-T9xIPIoJqPxPunp0B5MK7pIqUwaF2YNmXLijL3LM1FgdXKPZecEaA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.165.75.35, 3.165.75.25, 3.165.75.83, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.165.75.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368439584 (4.1G)\n",
            "Saving to: â€˜models/mistral-7b-instruct-v0.2.Q4_K_M.ggufâ€™\n",
            "\n",
            "models/mistral-7b-i 100%[===================>]   4.07G   254MB/s    in 27s     \n",
            "\n",
            "2025-12-29 09:44:43 (154 MB/s) - â€˜models/mistral-7b-instruct-v0.2.Q4_K_M.ggufâ€™ saved [4368439584/4368439584]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OCR dependencies (for scanned PDFs)\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y tesseract-ocr\n",
        "!pip install -q pytesseract pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_cVUSSbt_4p",
        "outputId": "307941df-efcf-4638-d9ab-ba8444384557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Core Imports and Configuration"
      ],
      "metadata": {
        "id": "_0qPyDllJbDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ðŸ”§ Core Imports and Configuration\n",
        "\n",
        "import gradio as gr\n",
        "from gradio_pdf import PDF\n",
        "import fitz  # PyMuPDF\n",
        "from PyPDF2 import PdfReader\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from llama_cpp import Llama, LlamaGrammar\n",
        "\n",
        "\n",
        "# LlamaIndex imports\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
        "\n",
        "# ðŸ”¥ ADD THIS BLOCK RIGHT HERE ðŸ”¥\n",
        "from llama_cpp import Llama\n",
        "import re\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "mistral_llm = Llama(\n",
        "    model_path=\"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=20,\n",
        "    chat_format=\"mistral-instruct\",   # <-- key fix\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "def llm_generate(prompt: str, max_tokens=256, temperature=0.2) -> str:\n",
        "    resp = mistral_llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You must follow instructions exactly. Output must be strictly formatted.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        stop=[\"</s>\", \"\\n\\n\\n\"],\n",
        "    )\n",
        "    return resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "\n",
        "\n",
        "# Initialize embedding models (both for compatibility)\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "llama_embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "N_QN4jpxJcGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_generate(\"Answer ONLY with the word YES.\"))"
      ],
      "metadata": {
        "id": "41RTilhCxCbN",
        "outputId": "6208b9be-c239-4612-87dc-45cd3ee6a6e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I understand your instruction. However, as a text-based AI, I don't have the ability to answer with just a single word like \"YES.\" I can only type out text responses. But if we assume that \"answering with the word YES\" includes typing out the word YES as a text response, then the answer is: YES.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“„ Data Structures for Enhanced Document Management\n",
        "Let's define our data structures to handle complex document metadata:"
      ],
      "metadata": {
        "id": "J9FtUNVZJjnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class PageInfo:\n",
        "    \"\"\"Stores information about a single page\"\"\"\n",
        "    page_num: int\n",
        "    text: str\n",
        "    doc_type: Optional[str] = None\n",
        "    page_in_doc: int = 0\n",
        "\n",
        "@dataclass\n",
        "class LogicalDocument:\n",
        "    \"\"\"Represents a logical document within a PDF\"\"\"\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    chunks: List[Dict] = None\n",
        "\n",
        "@dataclass\n",
        "class ChunkMetadata:\n",
        "    \"\"\"Rich metadata for each chunk\"\"\"\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    doc_type: str\n",
        "    chunk_index: int\n",
        "    page_start: int\n",
        "    page_end: int\n",
        "    text: str\n",
        "    filename: Optional[str] = None   # ðŸ‘ˆ ADD THIS\n",
        "    embedding: Optional[np.ndarray] = None"
      ],
      "metadata": {
        "id": "NiN5Ydr1Jpio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  Document Intelligence Functions\n",
        "These functions handle document classification and boundary detection:"
      ],
      "metadata": {
        "id": "rMN46qYfJrz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document_type(text: str, max_length: int = 1500) -> str:\n",
        "    \"\"\"\n",
        "    Classify the document type based on its content using Mistral (open-source).\n",
        "    \"\"\"\n",
        "    text_sample = text[:max_length] if len(text) > max_length else text\n",
        "\n",
        "    valid_types = [\n",
        "        'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n",
        "        'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n",
        "        'Insurance', 'Report', 'Letter', 'Form', 'ID Document',\n",
        "        'Medical', 'Other'\n",
        "    ]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a document classifier.\n",
        "\n",
        "Choose EXACTLY ONE category from this list:\n",
        "{\", \".join(valid_types)}\n",
        "\n",
        "Rules:\n",
        "- Output ONLY the category name\n",
        "- No explanation\n",
        "- If uncertain, output \"Other\"\n",
        "\n",
        "Document sample:\n",
        "{text_sample}\n",
        "\n",
        "Category:\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        raw = llm_generate(prompt, max_tokens=16, temperature=0.0)\n",
        "        doc_type = re.sub(r\"[^A-Za-z ]\", \"\", raw).strip().split()[0]\n",
        "\n",
        "        # Normalize strictly\n",
        "        for t in valid_types:\n",
        "            if doc_type.lower() == t.lower():\n",
        "                return t\n",
        "\n",
        "        return \"Other\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Classification error: {e}\")\n",
        "        return \"Other\"\n",
        "\n",
        "\n",
        "def detect_document_boundary(\n",
        "    prev_text: str,\n",
        "    curr_text: str,\n",
        "    current_doc_type: str = None\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Detect if two consecutive pages belong to the same document using Mistral.\n",
        "    Returns True if they are from the same document.\n",
        "    \"\"\"\n",
        "    if not prev_text or not curr_text:\n",
        "        return False\n",
        "\n",
        "    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n",
        "    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Determine whether these two pages belong to the SAME document.\n",
        "\n",
        "Consider:\n",
        "- Continuity of content\n",
        "- Formatting consistency\n",
        "- Topic coherence\n",
        "- Page numbers or headers\n",
        "\n",
        "Return JSON ONLY in this exact format:\n",
        "{{\n",
        "  \"same_document\": true or false,\n",
        "  \"confidence\": number between 0 and 1\n",
        "}}\n",
        "\n",
        "Current document type: {current_doc_type or \"Unknown\"}\n",
        "\n",
        "End of previous page:\n",
        "{prev_sample}\n",
        "\n",
        "Start of current page:\n",
        "{curr_sample}\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "\n",
        "        raw = llm_generate(prompt, max_tokens=120, temperature=0.0)\n",
        "\n",
        "        match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if not match:\n",
        "            # If model failed to follow format â†’ be conservative\n",
        "            return True\n",
        "\n",
        "        data = json.loads(match.group(0))\n",
        "\n",
        "        same_document = bool(data.get(\"same_document\", True))\n",
        "        confidence = float(data.get(\"confidence\", 0.0))\n",
        "\n",
        "        # HYBRID DECISION POLICY (HERE)\n",
        "        if confidence >= 0.85:\n",
        "          return same_document\n",
        "\n",
        "        # Low confidence â†’ keep pages together\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Boundary detection error: {e}\")\n",
        "        # safer default: keep pages together\n",
        "        return True"
      ],
      "metadata": {
        "id": "TSkSE_xPIZQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“‘ Advanced PDF Processing Pipeline\n",
        "Now let's build the enhanced PDF processing pipeline:"
      ],
      "metadata": {
        "id": "sz_YivbFJ10K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_analyze_pdf(pdf_file) -> Tuple[List[PageInfo], List[LogicalDocument]]:\n",
        "    \"\"\"\n",
        "    Extract text from PDF and perform intelligent document analysis.\n",
        "    Returns both page-level info and logical document groupings.\n",
        "    Supports various file types including scanned PDFs with OCR.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ“– Starting PDF extraction and analysis...\")\n",
        "\n",
        "    # Extract text from each page\n",
        "    if isinstance(pdf_file, dict) and \"content\" in pdf_file:\n",
        "        doc = fitz.open(stream=pdf_file[\"content\"], filetype=\"pdf\")\n",
        "    elif hasattr(pdf_file, \"read\"):\n",
        "        doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
        "    else:\n",
        "        doc = fitz.open(pdf_file)\n",
        "\n",
        "    pages_info = []\n",
        "    for i, page in enumerate(doc):\n",
        "        text = page.get_text()\n",
        "\n",
        "        # If no text found, try OCR (for scanned documents)\n",
        "        if not text.strip():\n",
        "            print(f\"  Page {i}: No text found, attempting OCR...\")\n",
        "            try:\n",
        "                # Convert page to image and perform OCR\n",
        "                pix = page.get_pixmap(dpi=300)\n",
        "                img_data = pix.tobytes(\"png\")\n",
        "                from PIL import Image\n",
        "                import pytesseract\n",
        "                import io\n",
        "\n",
        "                img = Image.open(io.BytesIO(img_data))\n",
        "                text = pytesseract.image_to_string(img)\n",
        "                print(f\"  Page {i}: OCR extracted {len(text)} characters\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Page {i}: OCR failed - {e}\")\n",
        "                text = \"\"\n",
        "\n",
        "        pages_info.append(PageInfo(page_num=i, text=text))\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "    if not pages_info:\n",
        "        raise ValueError(\"No text could be extracted from PDF\")\n",
        "\n",
        "    print(f\"âœ… Extracted {len(pages_info)} pages\")\n",
        "\n",
        "    # Perform document classification and boundary detection\n",
        "    print(\"ðŸ§  Analyzing document structure...\")\n",
        "    logical_docs = []\n",
        "    current_doc_type = None\n",
        "    current_doc_pages = []\n",
        "    doc_counter = 0\n",
        "\n",
        "    for i, page_info in enumerate(pages_info):\n",
        "        if i == 0:\n",
        "            # First page - classify document type\n",
        "            current_doc_type = classify_document_type(page_info.text)\n",
        "            page_info.doc_type = current_doc_type\n",
        "            page_info.page_in_doc = 0\n",
        "            current_doc_pages = [page_info]\n",
        "            print(f\"  Page {i}: New document detected - {current_doc_type}\")\n",
        "        else:\n",
        "            # Check if this page continues the previous document\n",
        "            prev_text = pages_info[i-1].text\n",
        "            is_same = detect_document_boundary(prev_text, page_info.text, current_doc_type)\n",
        "\n",
        "            if is_same:\n",
        "                # Continue current document\n",
        "                page_info.doc_type = current_doc_type\n",
        "                page_info.page_in_doc = len(current_doc_pages)\n",
        "                current_doc_pages.append(page_info)\n",
        "            else:\n",
        "                # New document detected - save previous and start new\n",
        "                logical_doc = LogicalDocument(\n",
        "                    doc_id=f\"doc_{doc_counter}\",\n",
        "                    doc_type=current_doc_type,\n",
        "                    page_start=current_doc_pages[0].page_num,\n",
        "                    page_end=current_doc_pages[-1].page_num,\n",
        "                    text=\"\\n\\n\".join([p.text for p in current_doc_pages])\n",
        "                )\n",
        "                logical_docs.append(logical_doc)\n",
        "                doc_counter += 1\n",
        "\n",
        "                # Start new document\n",
        "                current_doc_type = classify_document_type(page_info.text)\n",
        "                page_info.doc_type = current_doc_type\n",
        "                page_info.page_in_doc = 0\n",
        "                current_doc_pages = [page_info]\n",
        "                print(f\"  Page {i}: New document detected - {current_doc_type}\")\n",
        "\n",
        "    # Don't forget the last document\n",
        "    if current_doc_pages:\n",
        "        logical_doc = LogicalDocument(\n",
        "            doc_id=f\"doc_{doc_counter}\",\n",
        "            doc_type=current_doc_type,\n",
        "            page_start=current_doc_pages[0].page_num,\n",
        "            page_end=current_doc_pages[-1].page_num,\n",
        "            text=\"\\n\\n\".join([p.text for p in current_doc_pages])\n",
        "        )\n",
        "        logical_docs.append(logical_doc)\n",
        "\n",
        "    print(f\"âœ… Identified {len(logical_docs)} logical documents\")\n",
        "    for ld in logical_docs:\n",
        "        print(f\"   - {ld.doc_type}: Pages {ld.page_start}-{ld.page_end}\")\n",
        "\n",
        "    return pages_info, logical_docs"
      ],
      "metadata": {
        "id": "PIBKFsndJ5TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ‚ï¸ Intelligent Chunking with Metadata Preservation\n",
        "We'll provide two chunking approaches - our custom implementation and LlamaIndex's built-in capabilities:"
      ],
      "metadata": {
        "id": "LM89pqZjK_mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_document_with_metadata(logical_doc: LogicalDocument,\n",
        "                                chunk_size: int = 500,\n",
        "                                overlap: int = 100) -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Chunk a logical document while preserving rich metadata.\n",
        "    Uses sliding window with overlap for better context.\n",
        "    \"\"\"\n",
        "    chunks_metadata = []\n",
        "    words = logical_doc.text.split()\n",
        "\n",
        "    if len(words) <= chunk_size:\n",
        "        # Document is small enough to be a single chunk\n",
        "        chunk_meta = ChunkMetadata(\n",
        "            chunk_id=f\"{logical_doc.doc_id}_chunk_0\",\n",
        "            doc_id=logical_doc.doc_id,\n",
        "            doc_type=logical_doc.doc_type,\n",
        "            chunk_index=0,\n",
        "            page_start=logical_doc.page_start,\n",
        "            page_end=logical_doc.page_end,\n",
        "            text=logical_doc.text\n",
        "        )\n",
        "        chunks_metadata.append(chunk_meta)\n",
        "    else:\n",
        "        # Create overlapping chunks\n",
        "        stride = chunk_size - overlap\n",
        "        for i, start_idx in enumerate(range(0, len(words), stride)):\n",
        "            end_idx = min(start_idx + chunk_size, len(words))\n",
        "            chunk_text = ' '.join(words[start_idx:end_idx])\n",
        "\n",
        "            # Calculate which pages this chunk spans\n",
        "            # (simplified - in production, track more precisely)\n",
        "            chunk_position = start_idx / len(words)\n",
        "            page_range = logical_doc.page_end - logical_doc.page_start\n",
        "            relative_page = int(chunk_position * page_range)\n",
        "            chunk_page_start = logical_doc.page_start + relative_page\n",
        "            chunk_page_end = min(chunk_page_start + 1, logical_doc.page_end)\n",
        "\n",
        "            chunk_meta = ChunkMetadata(\n",
        "                chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "                doc_id=logical_doc.doc_id,\n",
        "                doc_type=logical_doc.doc_type,\n",
        "                chunk_index=i,\n",
        "                page_start=chunk_page_start,\n",
        "                page_end=chunk_page_end,\n",
        "                text=chunk_text\n",
        "            )\n",
        "            chunks_metadata.append(chunk_meta)\n",
        "\n",
        "            if end_idx >= len(words):\n",
        "                break\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "def chunk_with_llama_index(logical_doc: LogicalDocument,\n",
        "                           chunk_size: int = 500,\n",
        "                           chunk_overlap: int = 100) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Alternative: Use LlamaIndex's advanced chunking with metadata.\n",
        "    \"\"\"\n",
        "    # Create LlamaIndex document with metadata\n",
        "    doc = Document(\n",
        "        text=logical_doc.text,\n",
        "        metadata={\n",
        "            \"doc_id\": logical_doc.doc_id,\n",
        "            \"doc_type\": logical_doc.doc_type,\n",
        "            \"page_start\": logical_doc.page_start,\n",
        "            \"page_end\": logical_doc.page_end,\n",
        "            \"source\": f\"{logical_doc.doc_type}_document\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Use LlamaIndex's sentence splitter for better chunking\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        paragraph_separator=\"\\n\\n\",\n",
        "        separator=\" \",\n",
        "    )\n",
        "\n",
        "    # Create nodes (chunks) from document\n",
        "    nodes = splitter.get_nodes_from_documents([doc])\n",
        "\n",
        "    # Convert to our ChunkMetadata format for consistency\n",
        "    chunks_metadata = []\n",
        "    for i, node in enumerate(nodes):\n",
        "        chunk_meta = ChunkMetadata(\n",
        "            chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
        "            doc_id=logical_doc.doc_id,\n",
        "            doc_type=logical_doc.doc_type,\n",
        "            chunk_index=i,\n",
        "            page_start=node.metadata.get(\"page_start\", logical_doc.page_start),\n",
        "            page_end=node.metadata.get(\"page_end\", logical_doc.page_end),\n",
        "            text=node.text\n",
        "        )\n",
        "        chunks_metadata.append(chunk_meta)\n",
        "\n",
        "    return chunks_metadata\n",
        "\n",
        "def process_all_documents(logical_docs: List[LogicalDocument],\n",
        "                         use_llama_index: bool = False) -> List[ChunkMetadata]:\n",
        "    \"\"\"\n",
        "    Process all logical documents into chunks with metadata.\n",
        "    Can use either custom or LlamaIndex chunking.\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "\n",
        "    for logical_doc in logical_docs:\n",
        "        if use_llama_index:\n",
        "            chunks = chunk_with_llama_index(logical_doc)\n",
        "        else:\n",
        "            chunks = chunk_document_with_metadata(logical_doc)\n",
        "\n",
        "        logical_doc.chunks = chunks  # Store reference\n",
        "        all_chunks.extend(chunks)\n",
        "        print(f\"ðŸ“„ {logical_doc.doc_type}: Created {len(chunks)} chunks\")\n",
        "\n",
        "    return all_chunks"
      ],
      "metadata": {
        "id": "G6jI6IMnLCX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¯ Query Routing and Intelligent Retrieval"
      ],
      "metadata": {
        "id": "RESzcztYLEgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_query_document_type(query: str) -> Tuple[str, float]:\n",
        "    valid_types = [\n",
        "        'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n",
        "        'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n",
        "        'Insurance', 'Report', 'Letter', 'Form', 'ID Document',\n",
        "        'Medical', 'Other'\n",
        "    ]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Pick the MOST LIKELY document type that contains the answer.\n",
        "\n",
        "Valid types:\n",
        "{\", \".join(valid_types)}\n",
        "\n",
        "Query: \"{query}\"\n",
        "\n",
        "Return JSON ONLY in this exact format:\n",
        "{{\"type\":\"<one valid type>\",\"confidence\":<number between 0 and 1>}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        raw = llm_generate(prompt, max_tokens=80, temperature=0.0)\n",
        "\n",
        "        # Extract JSON object even if the model adds text\n",
        "        m = re.search(r'\\{.*\\}', raw, flags=re.DOTALL)\n",
        "        if not m:\n",
        "            return \"Other\", 0.0\n",
        "\n",
        "        obj = json.loads(m.group(0))\n",
        "        t = obj.get(\"type\", \"Other\")\n",
        "        c = float(obj.get(\"confidence\", 0.5))\n",
        "\n",
        "        # normalize\n",
        "        t_norm = \"Other\"\n",
        "        for vt in valid_types:\n",
        "            if str(t).lower() == vt.lower():\n",
        "                t_norm = vt\n",
        "                break\n",
        "        c = max(0.0, min(1.0, c))\n",
        "        return t_norm, c\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Query routing error: {e}\")\n",
        "        return \"Other\", 0.0\n",
        "\n",
        "\n",
        "class IntelligentRetriever:\n",
        "    \"\"\"\n",
        "    Advanced retrieval system with metadata filtering and query routing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.chunks_metadata = []\n",
        "        self.doc_type_indices = {}  # Separate indices per doc type\n",
        "\n",
        "    def build_indices(self, chunks_metadata: List[ChunkMetadata]):\n",
        "        \"\"\"\n",
        "        Build FAISS indices with document type segregation.\n",
        "        \"\"\"\n",
        "        print(\"ðŸ”¨ Building vector indices...\")\n",
        "        self.chunks_metadata = chunks_metadata\n",
        "\n",
        "        # Create embeddings for all chunks\n",
        "        texts = [chunk.text for chunk in chunks_metadata]\n",
        "        embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        # Store embeddings in metadata\n",
        "        for i, chunk in enumerate(chunks_metadata):\n",
        "            chunk.embedding = embeddings[i]\n",
        "\n",
        "        # Build main index\n",
        "        dim = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dim)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        # Build separate indices for each document type\n",
        "        doc_types = set(chunk.doc_type for chunk in chunks_metadata)\n",
        "        for doc_type in doc_types:\n",
        "            type_indices = [i for i, chunk in enumerate(chunks_metadata)\n",
        "                          if chunk.doc_type == doc_type]\n",
        "            if type_indices:\n",
        "                type_embeddings = embeddings[type_indices]\n",
        "                type_index = faiss.IndexFlatL2(dim)\n",
        "                type_index.add(type_embeddings)\n",
        "                self.doc_type_indices[doc_type] = {\n",
        "                    'index': type_index,\n",
        "                    'mapping': type_indices  # Maps back to original chunks\n",
        "                }\n",
        "\n",
        "        print(f\"âœ… Indexed {len(chunks_metadata)} chunks across {len(doc_types)} document types\")\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 4,\n",
        "                filter_doc_type: Optional[str] = None,\n",
        "                auto_route: bool = True) -> List[Tuple[ChunkMetadata, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant chunks with optional filtering and routing.\n",
        "        Returns chunks with relevance scores.\n",
        "        \"\"\"\n",
        "        query_embedding = embed_model.encode([query])\n",
        "\n",
        "        # Determine which index to search\n",
        "        if filter_doc_type and filter_doc_type in self.doc_type_indices:\n",
        "            # Use filtered index\n",
        "            type_data = self.doc_type_indices[filter_doc_type]\n",
        "            D, I = type_data['index'].search(query_embedding, k)\n",
        "            # Map back to original chunks\n",
        "            chunk_indices = [type_data['mapping'][i] for i in I[0]]\n",
        "            distances = D[0]\n",
        "        elif auto_route:\n",
        "            # Predict best document type\n",
        "            predicted_type, confidence = predict_query_document_type(query)\n",
        "            print(f\"ðŸŽ¯ Query routed to: {predicted_type} (confidence: {confidence:.2f})\")\n",
        "\n",
        "            if confidence > 0.7 and predicted_type in self.doc_type_indices:\n",
        "                # High confidence - use specific index\n",
        "                type_data = self.doc_type_indices[predicted_type]\n",
        "                D, I = type_data['index'].search(query_embedding, k)\n",
        "                chunk_indices = [type_data['mapping'][i] for i in I[0]]\n",
        "                distances = D[0]\n",
        "            else:\n",
        "                # Low confidence - search all\n",
        "                D, I = self.index.search(query_embedding, k)\n",
        "                chunk_indices = I[0]\n",
        "                distances = D[0]\n",
        "        else:\n",
        "            # Search all chunks\n",
        "            D, I = self.index.search(query_embedding, k)\n",
        "            chunk_indices = I[0]\n",
        "            distances = D[0]\n",
        "\n",
        "        # Convert distances to similarity scores (inverse)\n",
        "        max_dist = max(distances) if len(distances) > 0 else 1.0\n",
        "        scores = [(max_dist - d) / max_dist for d in distances]\n",
        "\n",
        "        results = [(self.chunks_metadata[i], scores[idx])\n",
        "                  for idx, i in enumerate(chunk_indices)]\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "Tqd7HSzPLMVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¬ Enhanced Answer Generation with Source Attribution"
      ],
      "metadata": {
        "id": "fT4Dws--LN2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_query_signals(query: str) -> List[str]:\n",
        "    prompt = f\"\"\"\n",
        "Extract 3â€“5 key semantic signals from this question.\n",
        "Signals should be nouns or short phrases likely to appear verbatim in documents.\n",
        "\n",
        "Return JSON list ONLY.\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Example output:\n",
        "[\"total\", \"monthly\", \"payment\"]\n",
        "\"\"\"\n",
        "    raw = llm_generate(prompt, max_tokens=80, temperature=0.0)\n",
        "    return json.loads(re.search(r\"\\[.*\\]\", raw).group(0))"
      ],
      "metadata": {
        "id": "cTBgKVGwa2i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_boost(retrieved_chunks, query_signals):\n",
        "    boosted = []\n",
        "    for chunk, score in retrieved_chunks:\n",
        "        text = chunk.text.lower()\n",
        "        overlap = sum(1 for s in query_signals if s.lower() in text)\n",
        "\n",
        "        if overlap > 0:\n",
        "            score = min(score + 0.05 * overlap, 1.0)\n",
        "\n",
        "        boosted.append((chunk, score))\n",
        "    return boosted"
      ],
      "metadata": {
        "id": "6JRAYn1Ba2Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_sources(\n",
        "    query: str,\n",
        "    retrieved_chunks: List[Tuple[ChunkMetadata, float]]\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Generate answer with detailed source attribution using Mistral.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1ï¸âƒ£ Extract semantic intent from the query (document-agnostic)\n",
        "    query_signals = extract_query_signals(query)\n",
        "\n",
        "    # 2ï¸âƒ£ Apply semantic boosting BEFORE filtering\n",
        "    retrieved_chunks = semantic_boost(retrieved_chunks, query_signals)\n",
        "\n",
        "    # 3ï¸âƒ£ Remove very low-relevance chunks (noise)\n",
        "    retrieved_chunks = [\n",
        "        (chunk, score)\n",
        "        for chunk, score in retrieved_chunks\n",
        "        if score > 0.2\n",
        "    ]\n",
        "\n",
        "    # Re-check after filtering\n",
        "    if not retrieved_chunks:\n",
        "        return {\n",
        "            'answer': \"I couldn't find relevant information to answer your question.\",\n",
        "            'sources': [],\n",
        "            'confidence': 0.0,\n",
        "            'chunks_used': 0\n",
        "        }\n",
        "\n",
        "    # 4ï¸âƒ£ Sort chunks by relevance (best evidence first)\n",
        "    retrieved_chunks = sorted(\n",
        "        retrieved_chunks,\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Build context and source list\n",
        "    context_parts = []\n",
        "    sources = []\n",
        "\n",
        "    for chunk_meta, score in retrieved_chunks:\n",
        "        context_parts.append(\n",
        "            f\"[SOURCE | {chunk_meta.doc_type} | Pages {chunk_meta.page_start}-{chunk_meta.page_end} | Chunk {chunk_meta.chunk_index}]\"\n",
        "        )\n",
        "        context_parts.append(chunk_meta.text)\n",
        "        context_parts.append(\"\")\n",
        "\n",
        "\n",
        "        sources.append({\n",
        "            'doc_type': chunk_meta.doc_type,\n",
        "            'filename': chunk_meta.filename,   # ðŸ‘ˆ ADD THIS\n",
        "            'pages': f\"{chunk_meta.page_start}-{chunk_meta.page_end}\",\n",
        "            'chunk': chunk_meta.chunk_index,\n",
        "            'relevance': f\"{score:.2%}\",\n",
        "            'preview': chunk_meta.text[:120].replace(\"\\n\", \" \") + \"...\"\n",
        "        })\n",
        "\n",
        "\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a document question-answering assistant.\n",
        "\n",
        "STRICT RULES:\n",
        "1. Use ONLY the information in the SOURCES.\n",
        "2. If the answer is not present, say:\n",
        "   \"I don't have enough information in the provided documents to answer that.\"\n",
        "3. Cite facts inline using (DocumentType pXâ€“Y).\n",
        "\n",
        "ANSWER RULES:\n",
        "- If multiple numbers exist, choose the value explicitly labeled as TOTAL\n",
        "- Ignore line items, fees, prepaid amounts, or partial components\n",
        "- Prefer headings like \"TOTAL ESTIMATED MONTHLY PAYMENT\"\n",
        "\n",
        "SOURCES:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{query}\n",
        "\n",
        "ANSWER (with citations):\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        answer = llm_generate(\n",
        "            prompt,\n",
        "            max_tokens=350,\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        # avg_score = sum(score for _, score in retrieved_chunks) / len(retrieved_chunks)\n",
        "        confidence = max(score for _, score in retrieved_chunks)\n",
        "\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': sources,\n",
        "            'confidence': float(confidence),\n",
        "            'chunks_used': len(retrieved_chunks)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Answer generation error: {e}\")\n",
        "        return {\n",
        "            'answer': f\"Error generating answer: {str(e)}\",\n",
        "            'sources': sources,\n",
        "            'confidence': 0.0,\n",
        "            'chunks_used': len(retrieved_chunks)\n",
        "        }"
      ],
      "metadata": {
        "id": "IITpvhTeLRsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ—ï¸ Enhanced Document Store"
      ],
      "metadata": {
        "id": "VbvV4zsCLTEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedDocumentStore:\n",
        "    \"\"\"\n",
        "    Manages the complete document processing and retrieval pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pages_info = []\n",
        "        self.logical_docs = []\n",
        "        self.chunks_metadata = []\n",
        "        self.retriever = IntelligentRetriever()\n",
        "        self.is_ready = False\n",
        "        self.processing_stats = {}\n",
        "        self.filename = None\n",
        "\n",
        "    def process_pdf(self, pdf_file, filename: str = \"document.pdf\"):\n",
        "        \"\"\"\n",
        "        Complete PDF processing pipeline.\n",
        "        \"\"\"\n",
        "        self.filename = filename\n",
        "        self.is_ready = False\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Extract and analyze PDF\n",
        "            self.pages_info, self.logical_docs = extract_and_analyze_pdf(pdf_file)\n",
        "\n",
        "            # Chunk documents with metadata\n",
        "            self.chunks_metadata = process_all_documents(self.logical_docs)\n",
        "\n",
        "            # Attach filename to every chunk\n",
        "            for chunk in self.chunks_metadata:\n",
        "                chunk.filename = self.filename\n",
        "\n",
        "            # Build retrieval indices\n",
        "            self.retriever.build_indices(self.chunks_metadata)\n",
        "\n",
        "            # Calculate processing statistics\n",
        "            process_time = (datetime.now() - start_time).total_seconds()\n",
        "            self.processing_stats = {\n",
        "                'filename': filename,\n",
        "                'total_pages': len(self.pages_info),\n",
        "                'documents_found': len(self.logical_docs),\n",
        "                'total_chunks': len(self.chunks_metadata),\n",
        "                'document_types': list(set(doc.doc_type for doc in self.logical_docs)),\n",
        "                'processing_time': f\"{process_time:.1f}s\"\n",
        "            }\n",
        "\n",
        "            self.is_ready = True\n",
        "            return True, self.processing_stats\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, {'error': str(e)}\n",
        "\n",
        "    def query(self, question: str, filter_type: Optional[str] = None,\n",
        "             auto_route: bool = True, k: int = 4) -> Dict:\n",
        "        \"\"\"\n",
        "        Query the document store.\n",
        "        \"\"\"\n",
        "        if not self.is_ready:\n",
        "            return {\n",
        "                'answer': \"Please upload and process a PDF first.\",\n",
        "                'sources': [],\n",
        "                'confidence': 0.0\n",
        "            }\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        retrieved = self.retriever.retrieve(\n",
        "            question, k=k,\n",
        "            filter_doc_type=filter_type,\n",
        "            auto_route=auto_route\n",
        "        )\n",
        "\n",
        "        # Generate answer with sources\n",
        "        result = generate_answer_with_sources(question, retrieved)\n",
        "        result['filter_used'] = filter_type or ('auto' if auto_route else 'none')\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_document_structure(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get the document structure for UI display.\n",
        "        \"\"\"\n",
        "        if not self.logical_docs:\n",
        "            return []\n",
        "\n",
        "        structure = []\n",
        "        for doc in self.logical_docs:\n",
        "            structure.append({\n",
        "                'id': doc.doc_id,\n",
        "                'type': doc.doc_type,\n",
        "                'pages': f\"{doc.page_start + 1}-{doc.page_end + 1}\",  # 1-indexed for UI\n",
        "                'chunks': len(doc.chunks) if doc.chunks else 0,\n",
        "                'preview': doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text\n",
        "            })\n",
        "\n",
        "        return structure"
      ],
      "metadata": {
        "id": "lyy5OLSTLViM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¨ Gradio Interface with Enhanced Features\n",
        "Now let's create the sophisticated Gradio interface:"
      ],
      "metadata": {
        "id": "sbeCC7ItLWpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global store instance\n",
        "doc_store = EnhancedDocumentStore()\n",
        "\n",
        "def process_pdf_handler(pdf_file):\n",
        "    \"\"\"Handle PDF upload and processing.\"\"\"\n",
        "    if pdf_file is None:\n",
        "        return \"âš ï¸ Please upload a PDF file\", None, gr.update(choices=[\"All\"])\n",
        "\n",
        "    # Process the PDF\n",
        "    success, stats = doc_store.process_pdf(pdf_file,\n",
        "                                          filename=getattr(pdf_file, 'name', 'document.pdf'))\n",
        "\n",
        "    if success:\n",
        "        # Prepare status message\n",
        "        status_msg = f\"\"\"\n",
        "        âœ… **Successfully Processed:**\n",
        "        - ðŸ“„ File: {stats['filename']}\n",
        "        - ðŸ“‘ Pages: {stats['total_pages']}\n",
        "        - ðŸ“š Documents Found: {stats['documents_found']}\n",
        "        - ðŸ§© Chunks Created: {stats['total_chunks']}\n",
        "        - ðŸ·ï¸ Types: {', '.join(stats['document_types'])}\n",
        "        - â±ï¸ Time: {stats['processing_time']}\n",
        "        \"\"\"\n",
        "\n",
        "        # Get document structure for display\n",
        "        structure = doc_store.get_document_structure()\n",
        "        structure_display = \"\\n\".join([\n",
        "            f\"â€¢ **{doc['type']}** (Pages {doc['pages']}): {doc['chunks']} chunks\"\n",
        "            for doc in structure\n",
        "        ])\n",
        "\n",
        "        # Update filter choices\n",
        "        doc_types = [\"All\"] + stats['document_types']\n",
        "\n",
        "        return status_msg, structure_display, gr.update(choices=doc_types, value=\"All\")\n",
        "\n",
        "    else:\n",
        "        return f\"âŒ Error: {stats.get('error', 'Unknown error')}\", \"\", gr.update(choices=[\"All\"])\n",
        "\n",
        "def chat_handler(message, history, doc_filter, auto_route, num_chunks):\n",
        "    \"\"\"Handle chat interactions.\"\"\"\n",
        "    if not doc_store.is_ready:\n",
        "        response = \"ðŸ“š Please upload and process a PDF document first.\"\n",
        "        return history + [\n",
        "            {\"role\": \"user\", \"content\": message},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "\n",
        "\n",
        "    # Query the document store\n",
        "    filter_type = None if doc_filter == \"All\" else doc_filter\n",
        "    result = doc_store.query(\n",
        "        message,\n",
        "        filter_type=filter_type,\n",
        "        auto_route=auto_route and filter_type is None,\n",
        "        k=num_chunks\n",
        "    )\n",
        "\n",
        "    # Format response with sources\n",
        "    response = f\"{result['answer']}\\n\\n\"\n",
        "\n",
        "    if result['sources']:\n",
        "        response += \"ðŸ“ **Sources:**\\n\"\n",
        "        for src in result['sources']:\n",
        "            response += f\"â€¢ {src['doc_type']} (Pages {src['pages']}) - Relevance: {src['relevance']}\\n\"\n",
        "\n",
        "    # response += f\"\\n*Confidence: {result['confidence']:.1%} | Filter: {result['filter_used']}*\"\n",
        "    response += (\n",
        "    f\"\\n*Confidence: {result['confidence']:.1%} \"\n",
        "    f\"(based on retrieval similarity across {len(result['sources'])} chunks) \"\n",
        "    f\"| Filter: {result['filter_used']}*\"\n",
        "    )\n",
        "\n",
        "    return history + [\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "    {\"role\": \"assistant\", \"content\": response}\n",
        "    ]\n",
        "\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Create the enhanced Gradio interface with unified single-tab layout.\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Enhanced Document Q&A\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # Enhanced Document Q&A System\n",
        "        ### Intelligent Multi-Document Analysis with Advanced RAG Pipeline\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            # Left side - PDF preview and upload\n",
        "            with gr.Column(scale=2):\n",
        "                pdf_input = gr.File(\n",
        "                    label=\"ðŸ“„ Upload PDF\",\n",
        "                    file_types=[\".pdf\"]\n",
        "                )\n",
        "\n",
        "                pdf_preview = gr.Markdown()\n",
        "\n",
        "\n",
        "                with gr.Row():\n",
        "                    process_btn = gr.Button(\n",
        "                        \"ðŸ”„ Process Document\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\",\n",
        "                        scale=2\n",
        "                    )\n",
        "                    clear_all_btn = gr.Button(\n",
        "                        \"ðŸ—‘ï¸ Clear All\",\n",
        "                        variant=\"secondary\",\n",
        "                        size=\"lg\",\n",
        "                        scale=1\n",
        "                    )\n",
        "\n",
        "            # Middle - Document info and settings\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ðŸ“Š Document Info\")\n",
        "                status_output = gr.Markdown(\n",
        "                    value=\"â³ Waiting for PDF upload...\"\n",
        "                )\n",
        "\n",
        "                structure_output = gr.Markdown(\n",
        "                    value=\"\",\n",
        "                    label=\"Document Structure\"\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### âš™ï¸ Settings\")\n",
        "\n",
        "                doc_filter = gr.Dropdown(\n",
        "                    choices=[\"All\"],\n",
        "                    value=\"All\",\n",
        "                    label=\"ðŸ·ï¸ Document Type Filter\",\n",
        "                    info=\"Filter search to specific document type\"\n",
        "                )\n",
        "\n",
        "                auto_route = gr.Checkbox(\n",
        "                    value=True,\n",
        "                    label=\"ðŸŽ¯ Auto-Route Queries\",\n",
        "                    info=\"Automatically detect relevant document type\"\n",
        "                )\n",
        "\n",
        "                num_chunks = gr.Slider(\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    value=4,\n",
        "                    step=1,\n",
        "                    label=\"ðŸ“Š Chunks to Retrieve\"\n",
        "                )\n",
        "\n",
        "            # Right side - Chat interface\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### ðŸ’¬ Ask Questions\")\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"Conversation\",\n",
        "                    height=500,\n",
        "                    elem_id=\"chatbot\",\n",
        "                    show_label=False\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    msg_input = gr.Textbox(\n",
        "                        label=\"Ask a question\",\n",
        "                        placeholder=\"e.g., What are the payment terms? What is the total amount?\",\n",
        "                        scale=4,\n",
        "                        show_label=False\n",
        "                    )\n",
        "                    send_btn = gr.Button(\"ðŸ“¤ Send\", scale=1, variant=\"primary\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    clear_chat_btn = gr.Button(\"ðŸ—‘ï¸ Clear Chat\", size=\"sm\", scale=1)\n",
        "                    example_btn1 = gr.Button(\"ðŸ“ What's the summary?\", size=\"sm\", scale=1)\n",
        "                    example_btn2 = gr.Button(\"ðŸ’° Find amounts\", size=\"sm\", scale=1)\n",
        "\n",
        "        # Status bar at the bottom\n",
        "        with gr.Row():\n",
        "            status_bar = gr.Markdown(\n",
        "                value=\"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\",\n",
        "                elem_id=\"status_bar\"\n",
        "            )\n",
        "\n",
        "        # Event handlers\n",
        "        def update_status_bar():\n",
        "            \"\"\"Update the status bar with current statistics.\"\"\"\n",
        "            if doc_store.is_ready:\n",
        "                stats = doc_store.processing_stats\n",
        "                cache_rate = 0\n",
        "                if hasattr(doc_store.retriever, 'total_queries') and doc_store.retriever.total_queries > 0:\n",
        "                    cache_rate = (doc_store.retriever.cache_hits / doc_store.retriever.total_queries) * 100\n",
        "\n",
        "                return f\"**Status:** âœ… Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)} | **Cache Rate:** {cache_rate:.0f}%\"\n",
        "            return \"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\"\n",
        "\n",
        "        def clear_all():\n",
        "            \"\"\"Clear everything and reset the interface.\"\"\"\n",
        "            global doc_store\n",
        "            doc_store = EnhancedDocumentStore()\n",
        "            return (\n",
        "                None,  # pdf_input\n",
        "                \"â³ Waiting for PDF upload...\",  # status_output\n",
        "                \"\",  # structure_output\n",
        "                gr.update(choices=[\"All\"], value=\"All\"),  # doc_filter\n",
        "                [],  # chatbot\n",
        "                \"\",  # msg_input\n",
        "                update_status_bar()  # status_bar\n",
        "            )\n",
        "\n",
        "        # Process PDF handler with status bar update\n",
        "        def process_pdf_with_status(pdf_file):\n",
        "            status, structure, filter_update = process_pdf_handler(pdf_file)\n",
        "            status_bar_text = update_status_bar()\n",
        "            return status, structure, filter_update, status_bar_text\n",
        "\n",
        "        # Chat handler with status bar update\n",
        "        def chat_with_status(message, history, doc_filter, auto_route, num_chunks):\n",
        "            new_history = chat_handler(message, history, doc_filter, auto_route, num_chunks)\n",
        "            status_bar_text = update_status_bar()\n",
        "            return new_history, status_bar_text\n",
        "\n",
        "        # Example question handlers\n",
        "        def ask_summary(history):\n",
        "          history = history or []\n",
        "          response = chat_handler(\n",
        "              \"Can you provide a summary of the main points in this document?\",\n",
        "              history,\n",
        "              doc_filter.value,\n",
        "              auto_route.value,\n",
        "              num_chunks.value\n",
        "          )\n",
        "          return response\n",
        "\n",
        "        def ask_amounts(history):\n",
        "          history = history or []\n",
        "          response = chat_handler(\n",
        "              \"What are all the monetary amounts or financial figures mentioned?\",\n",
        "              history,\n",
        "              doc_filter.value,\n",
        "              auto_route.value,\n",
        "              num_chunks.value\n",
        "          )\n",
        "          return response\n",
        "\n",
        "\n",
        "\n",
        "        # Wire up all the events\n",
        "        process_btn.click(\n",
        "            fn=process_pdf_with_status,\n",
        "            inputs=[pdf_input],\n",
        "            outputs=[status_output, structure_output, doc_filter, status_bar]\n",
        "        )\n",
        "\n",
        "        clear_all_btn.click(\n",
        "            fn=clear_all,\n",
        "            outputs=[pdf_input, status_output, structure_output, doc_filter,\n",
        "                    chatbot, msg_input, status_bar]\n",
        "        )\n",
        "\n",
        "        # Chat interactions\n",
        "        msg_input.submit(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],\n",
        "            outputs=[chatbot, status_bar]\n",
        "        ).then(\n",
        "            lambda: \"\",\n",
        "            outputs=[msg_input]\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            fn=chat_with_status,\n",
        "            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],\n",
        "            outputs=[chatbot, status_bar]\n",
        "        ).then(\n",
        "            lambda: \"\",\n",
        "            outputs=[msg_input]\n",
        "        )\n",
        "\n",
        "        clear_chat_btn.click(\n",
        "            lambda: [],\n",
        "            outputs=[chatbot]\n",
        "        )\n",
        "\n",
        "        example_btn1.click(\n",
        "            fn=ask_summary,\n",
        "            inputs=[chatbot],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            fn=update_status_bar,\n",
        "            outputs=[status_bar]\n",
        "        )\n",
        "\n",
        "        example_btn2.click(\n",
        "            fn=ask_amounts,\n",
        "            inputs=[chatbot],\n",
        "            outputs=[chatbot]\n",
        "        ).then(\n",
        "            fn=update_status_bar,\n",
        "            outputs=[status_bar]\n",
        "        )\n",
        "\n",
        "        # Auto-process when PDF is uploaded\n",
        "        pdf_input.change(\n",
        "            fn=process_pdf_with_status,\n",
        "            inputs=[pdf_input],\n",
        "            outputs=[status_output, structure_output, doc_filter, status_bar]\n",
        "        )\n",
        "\n",
        "\n",
        "    return demo"
      ],
      "metadata": {
        "id": "k7wXGcnJN6rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = create_interface()\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7fb296850f654f6384978e623242a75d",
            "0f90f63edba141d9a1435165045e3fb2",
            "412094a3175b479e82253f43adf2945d",
            "e4d985ca9eae4e38af416fd31e7eb8d7",
            "bf6a525760a84c369cc6531f440f4952",
            "71b362ff79d0443a89c944284757eb0a",
            "bb707a104d5248f2b0768ba68e9cac4c",
            "4cdc0dd00bee4d2ebddb7bde4bc6fb19",
            "ca218ac0825f43828dc4793caabe62d7",
            "b4ff3123acfe4545a1c4bbe14bab242c",
            "f5ca1de4326d41979fc06c9d0de03711",
            "5e146462e98a4a81b18396b5f0a38466",
            "9119df471c624dadad07d9d80796b136",
            "47db453f1dc24134856c55f7883b6838",
            "fc53bb56943b4ca88a0dbee4a349d8bb",
            "d6ea73dee0ec4e5ba708edf20ceb7ac4",
            "7aaac32129b34a6d9bf6a7e0a4f5e96c",
            "0150921ec61c4736ae68b94e174aec3a",
            "70ed490d95c04e95bdf4f437d67f26f6",
            "c7e216248da5423894bcc1f89b3f05fb",
            "814123040e1e4757934f8d5e0cfe4e49",
            "b7595c975f4e4251a64e22e5db107478"
          ]
        },
        "id": "LJJxZwTpN_pQ",
        "outputId": "37aa6617-dbfb-4f19-fe46-371ad1aa6913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3625924492.py:83: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
            "  with gr.Blocks(title=\"Enhanced Document Q&A\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://87126d8d3e2d9e2226.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://87126d8d3e2d9e2226.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“– Starting PDF extraction and analysis...\n",
            "âœ… Extracted 4 pages\n",
            "ðŸ§  Analyzing document structure...\n",
            "  Page 0: New document detected - Resume\n",
            "  Page 1: New document detected - Other\n",
            "  Page 2: New document detected - Other\n",
            "  Page 3: New document detected - Other\n",
            "âœ… Identified 4 logical documents\n",
            "   - Resume: Pages 0-0\n",
            "   - Other: Pages 1-1\n",
            "   - Other: Pages 2-2\n",
            "   - Other: Pages 3-3\n",
            "ðŸ“„ Resume: Created 1 chunks\n",
            "ðŸ“„ Other: Created 1 chunks\n",
            "ðŸ“„ Other: Created 1 chunks\n",
            "ðŸ“„ Other: Created 1 chunks\n",
            "ðŸ”¨ Building vector indices...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fb296850f654f6384978e623242a75d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Indexed 4 chunks across 2 document types\n",
            "Query routing error: Extra data: line 2 column 1 (char 46)\n",
            "ðŸŽ¯ Query routed to: Other (confidence: 0.00)\n",
            "ðŸ“– Starting PDF extraction and analysis...\n",
            "âœ… Extracted 2 pages\n",
            "ðŸ§  Analyzing document structure...\n",
            "  Page 0: New document detected - Contract\n",
            "âœ… Identified 1 logical documents\n",
            "   - Contract: Pages 0-1\n",
            "ðŸ“„ Contract: Created 1 chunks\n",
            "ðŸ”¨ Building vector indices...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e146462e98a4a81b18396b5f0a38466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Indexed 1 chunks across 1 document types\n",
            "ðŸŽ¯ Query routed to: Bank Statement (confidence: 0.80)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://87126d8d3e2d9e2226.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Slides link"
      ],
      "metadata": {
        "id": "yBE723kPmNVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.google.com/presentation/d/1mW1zGwmhKCAVbLKlylmGg19f-iTFcHWphiOrNgqym58/edit?usp=sharing"
      ],
      "metadata": {
        "id": "LMq45D8imQxy"
      }
    }
  ]
}